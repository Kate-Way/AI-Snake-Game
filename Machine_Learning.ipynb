{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoAxmSvdbS6VStsLqjSJjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kate-Way/AI-Snake-Game/blob/main/Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install a reinforcement learning library**"
      ],
      "metadata": {
        "id": "qr5i9XYU6F4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvUWUoHv39E5"
      },
      "outputs": [],
      "source": [
        "! pip install stable_baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "HQw2hgXSU_LE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "2DwNpSgUVIEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyglet"
      ],
      "metadata": {
        "id": "BTX6TD6lnsTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym \n",
        "import os\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "z69WJTSc4TA_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not mandatory, for large models only to regulate training process\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
      ],
      "metadata": {
        "id": "bDA7eRQuPSd_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Load Environment**"
      ],
      "metadata": {
        "id": "lIwRnFU1GhPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test environment"
      ],
      "metadata": {
        "id": "Lj4fnnwzKSPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n"
      ],
      "metadata": {
        "id": "5Fw3tGG_Gwcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9c7567-7f56-4107-ec86-5a94ff9807de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f4013975890>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "metadata": {
        "id": "dATlfmmiV0m-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make(\"CartPole-v0\"))\n",
        "\n",
        "episodes = 5\n",
        "for episode in range(1, episodes+1):\n",
        "    observation = env.reset()  # array of values, which actionto take to get max reward\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, done, info = env.step(action)  # unpacking step values \n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "oiZjJierV9fN",
        "outputId": "6ff4fe09-2f28-47f2-f9b1-1d1253a7c5df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:24.0\n",
            "Episode:2 Score:22.0\n",
            "Episode:3 Score:16.0\n",
            "Episode:4 Score:13.0\n",
            "Episode:5 Score:12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What environmental problem we're trying to solve (read more : https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
      ],
      "metadata": {
        "id": "Zx4GldYEAx44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0-push cart to left, 1-push cart to the right\n",
        "env.action_space.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqjdvx32ArXn",
        "outputId": "7c1bc968-4989-44d0-8e7b-dfaa2e246fa9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
        "env.observation_space "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slt6Iv82nyvf",
        "outputId": "fac38517-2c0d-4812-bc41-5cbdbc4f09d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Train an RL Model** (Model-free learning)"
      ],
      "metadata": {
        "id": "Dn6L5WljBssO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "# wrap environment (lambda: env - environment creation function)\n",
        "env = DummyVecEnv([lambda: env])  \n",
        "# MlpPolicy - neural network with standard network units (rules on how to operate environment) | verbose = 1 - we want to log results for that model\n",
        "# can pass bunch of different parameters, to look those up type PPO?? in code line and hit run\n",
        "model = PPO('MlpPolicy', env, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOpjNnhAn7PF",
        "outputId": "9feb602e-b8c9-4233-9b71-9140f365ad10"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=20000)\n",
        "# if you want to train yor model longer all you need to do is go and run it again "
      ],
      "metadata": {
        "id": "QgbHKy-jnLPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Save and Reload Model**"
      ],
      "metadata": {
        "id": "BmrhsBjwwx3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define log path\n",
        "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model')"
      ],
      "metadata": {
        "id": "6JbZWfclnME9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model after training\n",
        "model.save(PPO_path)"
      ],
      "metadata": {
        "id": "dJJoDvaKr2Kz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check my path\n",
        "PPO_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "qfAEvHNlsu1z",
        "outputId": "2e6b41c1-55f9-49de-e53c-cb96f4252d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Training/Saved Models/PPO_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "iShx2B1ryKdY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload model back into memory (pass full path to the model)\n",
        "model = PPO.load('Training/Saved Models/PPO_model', env=env)"
      ],
      "metadata": {
        "id": "yOhYhVpryMZb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Evaluate the model**"
      ],
      "metadata": {
        "id": "xFGTueQEEi1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "RzsrgNbgEnVU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_eval_policy = 10 (we're testing it for 10 episodes), rendering doesn't work in Colab - set it to False\n",
        "evaluate_policy(model, env, n_eval_episodes=10, render=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDOpZh_8ErjV",
        "outputId": "2282a8d2-5506-43f6-aef4-d37c999ecf3d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "_-7Jc-6KGE56"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Test model**"
      ],
      "metadata": {
        "id": "drmLD_ZMGJGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 5\n",
        "for episode in range(1, episodes+1):\n",
        "    obs = env.reset()  # observations for our observation space\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        env.render()\n",
        "        action, _states = model.predict(obs) # pass observations to prediction model (which action to take to max the revard)\n",
        "        obs, reward, done, info = env.step(action)  # unpacking step values \n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))\n",
        "\n",
        "# the score is great, but the vidoe won't show changes (we get 1 point reward for each time the pole doesn't fall)"
      ],
      "metadata": {
        "id": "15cpK6fBGMUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328416f5-5b76-4452-a8b6-844bb59bb67d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:[200.]\n",
            "Episode:2 Score:[200.]\n",
            "Episode:3 Score:[200.]\n",
            "Episode:4 Score:[200.]\n",
            "Episode:5 Score:[200.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cart position, cart velosity, pole angle, pole angular velosity\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixl-UoK0XAD3",
        "outputId": "b72fecd1-7897-4542-f38d-f7301bbc85af"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0145188 , -0.0014735 , -0.03104781, -0.0245294 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "qFDERUXNSuVb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = os.path.join('Training', 'Saved Models')\n",
        "log_path = os.path.join('Training', 'Logs')  #don't forget to make logs folder"
      ],
      "metadata": {
        "id": "kNooisuyPW88"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_log_path = os.path.join(log_path, 'PPO_3') #PPO_'x' x = how many times we run the model "
      ],
      "metadata": {
        "id": "NRfJGwASO-Dv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Viewing Logs in Tensorboard - RUN IN COMMAND LINE, NOT HERE (it won't stop running and everything will crash)**"
      ],
      "metadata": {
        "id": "XAgvETmTZZwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ##   tensorboard --logdir ={training_log_path}  \n",
        " # localhost:6006 will show you graphs of different train metrics \n",
        " # key metric - average reward, + episode lenghts (how long your agent lasts in the environment)"
      ],
      "metadata": {
        "id": "GKxH4g3bPeEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Adding a callback to the training Stage** - useful for large models"
      ],
      "metadata": {
        "id": "gz7-Rwg0heYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop training after sertain revard treshold \n",
        "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
        "\n",
        "eval_callback = EvalCallback(env, \n",
        "                             callback_on_new_best=stop_callback,  \n",
        "                             # every time there is a new best model stop_call back will run on it\n",
        "                             eval_freq=10000, \n",
        "                             best_model_save_path=save_path, \n",
        "                             verbose=1)"
      ],
      "metadata": {
        "id": "i7xrIKk_Z7Lj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
      ],
      "metadata": {
        "id": "hGk6TYKcj5HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs learning and saves best model\n",
        "model.learn(total_timesteps=20000, callback=eval_callback)"
      ],
      "metadata": {
        "id": "w8u8bqQ0j8jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "kf3AihdQlJs5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Changing Policies** (if you have a very specific reason to do so - there is a lot you can modify)"
      ],
      "metadata": {
        "id": "461WeT_WlRDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new neural network custom acta 4 units 128 layers in each, and value function with the same arcitecture \n",
        "net_arch=[dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
      ],
      "metadata": {
        "id": "P6FPd2dKlUP_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO('MlpPolicy', env, verbose = 1, policy_kwargs={'net_arch': net_arch})"
      ],
      "metadata": {
        "id": "scEUR_asmXVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=20000, callback=eval_callback)"
      ],
      "metadata": {
        "id": "GazF189pma0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Using an Alternate Algorithm**"
      ],
      "metadata": {
        "id": "AoY0eSK2nWjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN"
      ],
      "metadata": {
        "id": "1-S1xxTqnWN7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
      ],
      "metadata": {
        "id": "T7kOg6A1nc0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=20000, callback=eval_callback)"
      ],
      "metadata": {
        "id": "0-_vS3Q5nh_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
      ],
      "metadata": {
        "id": "1-FlucZhny31"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(dqn_path)"
      ],
      "metadata": {
        "id": "5H4n7rBTn1oB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN.load(dqn_path, env=env)"
      ],
      "metadata": {
        "id": "7AM7Og0qn4vO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_policy(model, env, n_eval_episodes=10, render=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ4f8eO1n73Z",
        "outputId": "cf87dfd8-3338-4f73-c28f-d6a23afa289a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9.5, 0.6708203932499369)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "-I-VJNjtoATS"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}